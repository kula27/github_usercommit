# This notebook converts csv file into spark dataframe and caculates average time between commits
#import org.apache.spark.sql.functions._
# File location and type
file_location = "/FileStore/tables/user_commits.csv"
file_type = "csv"

# CSV options
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)

#spark.sql("select unix_timestamp('2019-07-02 12:01:19') - unix_timestamp('df') DiffInSeconds")
#spark.sql("select to_timestamp('2019-06-24 12:01:19.000') as timestamp")
df.select(to_timestamp(lit('commit_datetime'),'MM-dd-yyyy HH:mm:ss')).show()
#commit_time = df.select(col("commit_datetime")).show()

#df1 = df.withColumn("commit_datetime_date",
#       to_timestamp(col("commit_datetime")))
#display(df)